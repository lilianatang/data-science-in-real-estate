{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sodapy\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/74/95fb7d45bbe7f1de43caac45d7dd4807ef1e15881564a00eef489a3bb5c6/sodapy-2.1.0-py2.py3-none-any.whl\n",
      "Collecting requests>=2.20.0 (from sodapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/96/144f70b972a9c0eabbd4391ef93ccd49d0f2747f4f6a2a2738e99e5adc65/requests-2.26.0-py2.py3-none-any.whl (62kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 4.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->sodapy) (1.22)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->sodapy) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->sodapy) (2019.11.28)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\" (from requests>=2.20.0->sodapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/53/b7f6126a2b9fd878b025fe3c40266cfaad696f312165008ce045bffa3fe7/charset_normalizer-2.0.4-py3-none-any.whl\n",
      "Installing collected packages: charset-normalizer, requests, sodapy\n",
      "  Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "Successfully installed charset-normalizer-2.0.4 requests-2.26.0 sodapy-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sodapy\n",
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n"
     ]
    }
   ],
   "source": [
    "client = Socrata('data.edmonton.ca',\n",
    "                  '47asXuyWaxWsAxOt3EGvqq40r',\n",
    "                  username='8fhzefz0bvnvu12f223cjk9e7',\n",
    "                  password='245x2cx2xwx08mvusu7aml8f4n94bvdlvm6a31k6miu3c8cpz0',\n",
    "                 timeout=30)\n",
    "batch_size = 3500\n",
    "batchs = 965\n",
    "count = 0\n",
    "for batch in range(batchs):\n",
    "    offset = batch * batch_size\n",
    "    results = client.get(\"qi6a-xuwt\", limit=batch_size, offset=offset)\n",
    "    results_df = pd.DataFrame.from_records(results)\n",
    "    columns = ['account_number', 'suite', 'lot_size', 'assessed_value', 'assessment_year', 'garage', 'house_number', 'mill_class_1', 'neighborhood_name', 'street_name', 'year_built', 'zoning']\n",
    "    df1 = pd.DataFrame(results_df, columns=columns)\n",
    "    \n",
    "    # Convert assessment year and value to numeric value\n",
    "    df1[\"assessment_year\"] = df1[\"assessment_year\"].astype(str).astype(int)\n",
    "    df1[\"assessed_value\"] = df1[\"assessed_value\"].astype(str).astype(float)\n",
    "    df1[\"lot_size\"] = df1[\"lot_size\"].astype(str).astype(float)\n",
    "    # Fill in missing data for house number with 0 & change its data type to integer\n",
    "    df1['house_number'] = df1['house_number'].fillna(0)\n",
    "    df1[\"house_number\"] = df1[\"house_number\"].astype(str).astype(int)\n",
    "    df1['year_built'] = df1['year_built'].fillna(0)\n",
    "    df1[\"year_built\"] = df1[\"year_built\"].astype(str).astype(int)\n",
    "    \n",
    "    # Define data type of each column to be converted to a pyspark dataframe\n",
    "    houseSchema = StructType([\n",
    "        StructField('account_number', StringType(), True),\n",
    "        StructField('suite', StringType(), True),\n",
    "        StructField(\"lot_size\", FloatType(), True),\n",
    "        StructField(\"assessed_value\", FloatType(), True),\n",
    "        StructField(\"assessment_year\", IntegerType(), True),\n",
    "        StructField(\"garage\", StringType(), True),\n",
    "        StructField(\"house_number\", IntegerType(), True),\n",
    "        StructField(\"mill_class_1\", StringType(), True),\n",
    "        StructField(\"neighborhood_name\", StringType(), True),\n",
    "        StructField(\"street_name\", StringType(), True),\n",
    "        StructField(\"year_built\", IntegerType(), True),\n",
    "        StructField(\"zoning\", StringType(), True)\n",
    "    ])\n",
    "    sparkDF_house_value=spark.createDataFrame(df1, schema=houseSchema)    \n",
    "    # Write the dataframe to a parquet file\n",
    "    sparkDF_house_value.write.mode('append').parquet(\"output/houses.parquet\")\n",
    "    count = count + 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "houses = spark.read.parquet(\"output/houses.parquet\")\n",
    "houses.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Question 1: Does residential or commercial real estate have higher Return on Investment (ROI)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "houses.createOrReplaceTempView(\"houses_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# find out how many years the dataset has\n",
    "spark.sql(\"SELECT distinct assessment_year FROM houses_table ORDER BY assessment_year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Other Residential = Multi-family (apartments & townhouses)\n",
    "# Residential with suite = Individual townhouse/ condo/ apartment\n",
    "final_df = spark.sql(\"SELECT 2012 AS year, mill_class_1, AVG(assessed_value) AS average_value FROM houses_table WHERE assessment_year = 2012 GROUP BY mill_class_1 \\\n",
    "        UNION ALL (SELECT 2020 AS year, mill_class_1, AVG(assessed_value) AS average_value FROM houses_table WHERE assessment_year = 2020 GROUP BY mill_class_1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "final_df.createOrReplaceTempView(\"houses_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * from houses_data_table WHERE year = 2012\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ROI_df = spark.sql(\"SELECT a.mill_class_1, a.average_value AS 2012_value, b.average_value AS 2020_value, (b.average_value - a.average_value)/a.average_value * 100 AS ROI FROM (SELECT mill_class_1, year, average_value from houses_data_table WHERE year = 2012) a \\\n",
    "        JOIN (SELECT mill_class_1, year, average_value from houses_data_table WHERE year = 2020) b ON a.mill_class_1 = b.mill_class_1 \\\n",
    "        WHERE a.mill_class_1 = 'RESIDENTIAL' OR a.mill_class_1 = 'OTHER RESIDENTIAL' ORDER BY (b.average_value - a.average_value)/a.average_value * 100 desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ROI_df.createOrReplaceTempView(\"ROI_table\")\n",
    "final_ROI_df = spark.sql(\"SELECT ROI, CASE WHEN mill_class_1 = 'OTHER RESIDENTIAL' THEN 'Multi family' ELSE 'Single family' END AS property_type FROM ROI_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "final_ROI_df = final_ROI_df.toPandas()\n",
    "final_ROI_df= final_ROI_df.sort_values('ROI',ascending=False)\n",
    "final_ROI_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Return the Series having unique values\n",
    "df_sorted_desc= final_ROI_df.sort_values('ROI',ascending=False)\n",
    "df_sorted_desc.head()\n",
    "plt.bar(\"property_type\", \"ROI\", data=final_ROI_df)\n",
    "# Labeling the axes\n",
    "plt.xlabel('Property Type')\n",
    "plt.ylabel('Return on Investment (%)')\n",
    "plt.title(\"Return on Investment for Property Types (2012 - 2020)\", size=14)\n",
    "plt.savefig(\"bar_plot_ROI_by_property_type.png\")\n",
    "# Dsiplay the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Question 2: Is Multi Family a Riskier Investment than Single Family?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performance of Multi Family & Single Family over years\n",
    "spark.sql(\"SELECT * from houses_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "performance_df = spark.sql(\"SELECT AVG(assessed_value) AS avg_value, assessment_year, mill_class_1 from houses_table WHERE mill_class_1 = 'RESIDENTIAL' OR mill_class_1 = 'OTHER RESIDENTIAL' GROUP BY assessment_year, mill_class_1 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "performance_df.createOrReplaceTempView(\"performance_table\")\n",
    "final_performance_df = spark.sql(\"SELECT avg_value, assessment_year, CASE WHEN mill_class_1 = 'OTHER RESIDENTIAL' THEN 'Multi family' ELSE 'Single family' END AS property_type FROM performance_table\")\n",
    "final_performance_df.createOrReplaceTempView(\"final_performance_table\")\n",
    "multi_family_performance = spark.sql(\"SELECT * FROM final_performance_table WHERE property_type='Multi family'\")\n",
    "single_family_performance = spark.sql(\"SELECT * FROM final_performance_table WHERE property_type='Single family'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "multi_family_performance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "multi_family_df = multi_family_performance.toPandas()\n",
    "multi_family_df= multi_family_df.sort_values('assessment_year',ascending=True)\n",
    "\n",
    "single_family_df = single_family_performance.toPandas()\n",
    "single_family_df= single_family_df.sort_values('assessment_year',ascending=True)\n",
    "# plot lines\n",
    "plt.plot(\"assessment_year\", \"avg_value\", data=multi_family_df, label = \"Multi family\")\n",
    "plt.plot(\"assessment_year\", \"avg_value\", data=single_family_df, label = \"Single family\")\n",
    "# Labeling the axes\n",
    "plt.xlabel('Assessment Year')\n",
    "plt.ylabel('Average Value ($)')\n",
    "plt.title(\"Average Value Over Years per Property Type in Edmonton\", size=14)\n",
    "plt.legend()\n",
    "plt.savefig(\"line_chart_by_property_type.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Question 3: How well can we predict a property's value? What aspects correlate well to the value of a property?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "final_prediction_df = spark.sql(\"SELECT CASE WHEN mill_class_1 = 'OTHER RESIDENTIAL' THEN 'Multi family' ELSE 'Single family' END AS property_type, lot_size, garage, street_name, year_built, zoning, CASE WHEN suite NOT LIKE '%NaN' THEN 'Condominium' ELSE 'Detached House' END AS type, assessed_value FROM houses_table WHERE assessment_year = 2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "final_prediction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "final_prediction_df.cache()\n",
    "final_prediction_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply OneHotEncoder to property_type, garage, street_name, zoning, and type columns\n",
    "# Apply StringIndexer to string columns first\n",
    "# import required libraries\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "property_type_indexer = StringIndexer(inputCol=\"property_type\", outputCol=\"propertyTypeIndex\")\n",
    "#Fits a model to the input dataset with optional parameters.\n",
    "final_prediction_df = property_type_indexer.fit(final_prediction_df).transform(final_prediction_df)\n",
    "final_prediction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a\n",
    "# https://medium.com/@nutanbhogendrasharma/feature-transformer-vectorassembler-in-pyspark-ml-feature-part-3-b3c2c3c93ee9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply OneHotEncoder to property_type, garage, street_name, zoning, and type columns\n",
    "# Apply StringIndexer to string columns first\n",
    "# import required libraries\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "garage_indexer = StringIndexer(inputCol=\"garage\", outputCol=\"garageIndex\")\n",
    "#Fits a model to the input dataset with optional parameters.\n",
    "final_prediction_df = garage_indexer.fit(final_prediction_df).transform(final_prediction_df)\n",
    "final_prediction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply OneHotEncoder to property_type, garage, street_name, zoning, and type columns\n",
    "# Apply StringIndexer to string columns first\n",
    "# import required libraries\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "street_name_indexer = StringIndexer(inputCol=\"street_name\", outputCol=\"streetNameIndex\")\n",
    "#Fits a model to the input dataset with optional parameters.\n",
    "final_prediction_df = street_name_indexer.fit(final_prediction_df).transform(final_prediction_df)\n",
    "final_prediction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply OneHotEncoder to property_type, garage, street_name, zoning, and type columns\n",
    "# Apply StringIndexer to string columns first\n",
    "# import required libraries\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "zoning_indexer = StringIndexer(inputCol=\"zoning\", outputCol=\"zoningIndex\")\n",
    "#Fits a model to the input dataset with optional parameters.\n",
    "final_prediction_df = zoning_indexer.fit(final_prediction_df).transform(final_prediction_df)\n",
    "final_prediction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply OneHotEncoder to property_type, garage, street_name, zoning, and type columns\n",
    "# Apply StringIndexer to string columns first\n",
    "# import required libraries\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "type_indexer = StringIndexer(inputCol=\"type\", outputCol=\"typeIndex\")\n",
    "#Fits a model to the input dataset with optional parameters.\n",
    "final_prediction_df = type_indexer.fit(final_prediction_df).transform(final_prediction_df)\n",
    "final_prediction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now apply OneHotEncoder to string columns\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "#onehotencoder to propertyTypeIndex\n",
    "onehotencoder_property_type_vector = OneHotEncoder(inputCol=\"propertyTypeIndex\", outputCol=\"property_type_vec\")\n",
    "final_prediction_df = onehotencoder_property_type_vector.transform(final_prediction_df)\n",
    "final_prediction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now apply OneHotEncoder to string columns\n",
    "# garage, street_name, zoning, and type\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "#onehotencoder to propertyTypeIndex\n",
    "onehotencoder_garage_vector = OneHotEncoder(inputCol=\"garageIndex\", outputCol=\"garage_vec\")\n",
    "final_prediction_df = onehotencoder_garage_vector.transform(final_prediction_df)\n",
    "final_prediction_df.show()\n",
    "\n",
    "# Now apply OneHotEncoder to string columns\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "#onehotencoder to propertyTypeIndex\n",
    "onehotencoder_street_name_vector = OneHotEncoder(inputCol=\"streetNameIndex\", outputCol=\"street_name_vec\")\n",
    "final_prediction_df = onehotencoder_street_name_vector.transform(final_prediction_df)\n",
    "final_prediction_df.show()\n",
    "\n",
    "# Now apply OneHotEncoder to string columns\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "#onehotencoder to propertyTypeIndex\n",
    "onehotencoder_zoning_vector = OneHotEncoder(inputCol=\"zoningIndex\", outputCol=\"zoning_vec\")\n",
    "final_prediction_df = onehotencoder_zoning_vector.transform(final_prediction_df)\n",
    "final_prediction_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#onehotencoder to propertyTypeIndex\n",
    "onehotencoder_type_vector = OneHotEncoder(inputCol=\"typeIndex\", outputCol=\"type_vec\")\n",
    "final_prediction_df = onehotencoder_type_vector.transform(final_prediction_df)\n",
    "\n",
    "# Feature transformer — VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "final_prediction_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputCols = [\n",
    "    'lot_size',\n",
    "    'year_built',\n",
    "    'propertyTypeIndex',\n",
    "    'garageIndex',\n",
    "    'streetNameIndex',\n",
    "    'zoningIndex',\n",
    "    'typeIndex',\n",
    "    'property_type_vec',\n",
    "    'garage_vec',\n",
    "    'street_name_vec',\n",
    "    'zoning_vec',\n",
    "    'type_vec'\n",
    "]\n",
    "outputCol = \"features\"\n",
    "df_va = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "final_prediction_df = df_va.setHandleInvalid(\"skip\").transform(final_prediction_df)\n",
    "final_prediction_df.select(['features']).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_df = final_prediction_df.select(['features','assessed_value'])\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split data set into training and set datasets\n",
    "splits = new_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply Gradient-boosted tree regression algorithm\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'assessed_value', maxIter=10,  maxBins=2846)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'assessed_value', 'features').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "gbt_evaluator = RegressionEvaluator(labelCol=\"assessed_value\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % int(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The Root Mean Squared Error of 7,508,308 is unacceptable. Although with almost 400000 data rows, the root mean squared error is still very high. Therefore, we are very close to conclude that the features we picked are not all important factors in determining the price of a home. Let's back it up through calculating the feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    " gbt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
